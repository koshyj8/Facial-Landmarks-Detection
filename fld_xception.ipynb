{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "from xml.etree import ElementTree as ET\n",
    "from skimage import io\n",
    "import albumentations as A\n",
    "from albumentations.augmentations import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    def __init__(self, preprocessor, train):\n",
    "        self.root_dir = r'datasets\\ibug_300W_large_face_landmark_dataset'\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.preprocessor = preprocessor\n",
    "        self.train = train\n",
    "        \n",
    "        tree = ET.parse(os.path.join(self.root_dir, f'labels_ibug_300W_{\"train\" if train else \"test\"}.xml'))\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        for filename in root[2]:\n",
    "            self.image_paths.append(os.path.join(self.root_dir, filename.attrib['file']))\n",
    "\n",
    "            self.crops.append(filename[0].attrib)\n",
    "\n",
    "            landmark = []\n",
    "            for num in range(68):\n",
    "                x_coordinate = int(filename[0][num].attrib['x'])\n",
    "                y_coordinate = int(filename[0][num].attrib['y'])\n",
    "                landmark.append([x_coordinate, y_coordinate])\n",
    "            self.landmarks.append(landmark)\n",
    "\n",
    "        self.landmarks = np.array(self.landmarks).astype('float32')\n",
    "\n",
    "        assert len(self.image_paths) == len(self.landmarks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = io.imread(self.image_paths[index], as_gray = False)\n",
    "        landmarks = self.landmarks[index]\n",
    "        \n",
    "        image, landmarks = self.preprocessor(image, landmarks, self.crops[index])\n",
    "\n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceAugmentation:\n",
    "    def __init__(self,\n",
    "                 image_dim,\n",
    "                 brightness,    \n",
    "                 contrast,\n",
    "                 saturation,\n",
    "                 hue,\n",
    "                 face_offset,\n",
    "                 crop_offset):\n",
    "        \n",
    "        self.image_dim = image_dim\n",
    "        self.face_offset = face_offset\n",
    "        self.crop_offset = crop_offset\n",
    "        self.transform = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "    \n",
    "    def offset_crop(self, image, landmarks, crops_coordinates):\n",
    "        left = int(crops_coordinates['left']) - self.face_offset\n",
    "        top = int(crops_coordinates['top']) - self.face_offset\n",
    "        width = int(crops_coordinates['width']) + (2 * self.face_offset)\n",
    "        height = int(crops_coordinates['height']) + (2 * self.face_offset)\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "        landmarks = landmarks - np.array([[left, top]])\n",
    "\n",
    "        new_dim = self.image_dim + self.crop_offset\n",
    "\n",
    "        image = TF.resize(image, (new_dim, new_dim))\n",
    "\n",
    "        landmarks[:, 0] *= new_dim / width\n",
    "        landmarks[:, 1] *= new_dim / height\n",
    "\n",
    "        return image, landmarks\n",
    "    \n",
    "    def random_face_crop(self, image, landmarks):\n",
    "        image = np.array(image)\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        top = np.random.randint(0, h - self.image_dim)\n",
    "        left = np.random.randint(0, w - self.image_dim)\n",
    "\n",
    "        image = image[top: top + self.image_dim, left: left + self.image_dim]\n",
    "\n",
    "        landmarks = landmarks - np.array([[left, top]])\n",
    "\n",
    "        return TF.to_pil_image(image), landmarks\n",
    "    \n",
    "    def __call__(self, image, landmarks, crops_coordinates):\n",
    "        image, landmarks = self.offset_crop(image, landmarks, crops_coordinates)\n",
    "\n",
    "        image, landmarks = self.random_face_crop(image, landmarks)\n",
    "\n",
    "        return self.transform(image), landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksAugmentation:\n",
    "    def __init__(self, rotation_limit):\n",
    "        self.rotation_limit = rotation_limit\n",
    "\n",
    "    def random_rotation(self, image, landmarks):\n",
    "        angle = np.random.uniform(-self.rotation_limit, self.rotation_limit)\n",
    "        landmarks_transformation = np.array([\n",
    "            [+np.cos(np.radians(angle)), -np.sin(np.radians(angle))], \n",
    "            [+np.sin(np.radians(angle)), +np.cos(np.radians(angle))]\n",
    "        ])\n",
    "        image = TF.rotate(image, angle)\n",
    "        landmarks = landmarks - 0.5\n",
    "        transformed_landmarks = np.matmul(landmarks, landmarks_transformation)\n",
    "        transformed_landmarks = transformed_landmarks + 0.5\n",
    "\n",
    "        return image, transformed_landmarks\n",
    "    \n",
    "    def __call__(self, image, landmarks):\n",
    "        image, landmarks = self.random_rotation(image, landmarks)\n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,\n",
    "                 image_dim,\n",
    "                 brightness,\n",
    "                 contrast,\n",
    "                 saturation,\n",
    "                 hue,\n",
    "                 angle,\n",
    "                 face_offset,\n",
    "                 crop_offset):\n",
    "        \n",
    "        self.image_dim = image_dim\n",
    "\n",
    "        self.landmarks_augmentation = LandmarksAugmentation(angle)\n",
    "        self.face_augmentation = FaceAugmentation(image_dim, brightness, contrast, saturation, hue, face_offset, crop_offset)\n",
    "    \n",
    "    def __call__(self, image, landmarks, crops_coordinates):\n",
    "        image = TF.to_pil_image(image)\n",
    "\n",
    "        image, landmarks = self.face_augmentation(image, landmarks, crops_coordinates)\n",
    "\n",
    "        landmarks = landmarks / np.array([*image.size])\n",
    "\n",
    "        image, landmarks = self.landmarks_augmentation(image, landmarks)\n",
    "\n",
    "        image = TF.to_grayscale(image)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "\n",
    "        image = (image - image.min())/(image.max() - image.min())\n",
    "        image = (2 * image) - 1\n",
    "\n",
    "        return image, torch.FloatTensor(landmarks.reshape(-1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(\n",
    "    image_dim = 128,\n",
    "    brightness = 0.24,\n",
    "    saturation = 0.3,\n",
    "    contrast = 0.15,\n",
    "    hue = 0.14,\n",
    "    angle = 14,\n",
    "    face_offset = 32,\n",
    "    crop_offset = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(image, landmarks):\n",
    "    plt.figure(figsize = (5, 5))\n",
    "    image = (image - image.min())/(image.max() - image.min())\n",
    "\n",
    "    landmarks = landmarks.view(-1, 2)\n",
    "    landmarks = (landmarks + 0.5) * preprocessor.image_dim\n",
    "\n",
    "    plt.imshow(image[0], cmap = 'gray')\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s = 25, c = 'dodgerblue')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_batch(images_list, landmarks_list, size = 14, shape = (6, 6), title = None, save = None):\n",
    "    fig = plt.figure(figsize = (size, size))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols = shape, axes_pad = 0.08)\n",
    "    for ax, image, landmarks in zip(grid, images_list, landmarks_list):\n",
    "        image = (image - image.min())/(image.max() - image.min())\n",
    "\n",
    "        landmarks = landmarks.view(-1, 2)\n",
    "        landmarks = (landmarks + 0.5) * preprocessor.image_dim\n",
    "        landmarks = landmarks.numpy().tolist()\n",
    "        landmarks = np.array([(x, y) for (x, y) in landmarks if 0 <= x <= preprocessor.image_dim and 0 <= y <= preprocessor.image_dim])\n",
    "\n",
    "        ax.imshow(image[0], cmap = 'gray')\n",
    "        ax.scatter(landmarks[:, 0], landmarks[:, 1], s = 10, c = 'dodgerblue')\n",
    "        ax.axis('off')\n",
    "\n",
    "    if title:\n",
    "        print(title)\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FaceLandmarksDataset(preprocessor,train=True)\n",
    "test_dataset = FaceLandmarksDataset(preprocessor,train=False)\n",
    "\n",
    "test_dataset_length = len(test_dataset)\n",
    "\n",
    "val_size = int(0.8 * test_dataset_length)\n",
    "test_size = test_dataset_length - val_size\n",
    "\n",
    "val_dataset, final_test_dataset = random_split(test_dataset, [val_size, test_size])\n",
    "\n",
    "validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "testing_loader = DataLoader(final_test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "training_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeperableConv2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n",
    "        super(DepthwiseSeperableConv2d, self).__init__()\n",
    "\n",
    "        self.depthwise = nn.Conv2d(input_channels, input_channels, kernel_size, groups = input_channels, bias = False, **kwargs)\n",
    "        self.pointwise = nn.Conv2d(input_channels, output_channels, 1, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntryBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EntryBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv3_residual = nn.Sequential(\n",
    "            DepthwiseSeperableConv2d(64, 64, 3, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(64, 128, 3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(3, stride = 2, padding = 1),\n",
    "        )\n",
    "\n",
    "        self.conv3_direct = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 1, stride = 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        self.conv4_residual = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(128, 128, 3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(128, 256, 3, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(3, stride = 2, padding = 1)\n",
    "        )\n",
    "\n",
    "        self.conv4_direct = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 1, stride = 2),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        residual = self.conv3_residual(x)\n",
    "        direct = self.conv3_direct(x)\n",
    "        x = residual + direct\n",
    "        \n",
    "        residual = self.conv4_residual(x)\n",
    "        direct = self.conv4_direct(x)\n",
    "        x = residual + direct\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiddleBasicBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MiddleBasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(256, 256, 3, padding = 1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(256, 256, 3, padding = 1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(256, 256, 3, padding = 1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.conv3(residual)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    def __init__(self, blocks_n):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(*[MiddleBasicBlock() for _ in range(blocks_n)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExitBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExitBlock, self).__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(256, 256, 3, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(256, 512, 3, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(3, stride = 2, padding = 1)\n",
    "        )\n",
    "\n",
    "        self.direct = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 1, stride = 2),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            DepthwiseSeperableConv2d(512, 512, 3, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            DepthwiseSeperableConv2d(512, 1024, 3, padding = 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        direct = self.direct(x)\n",
    "        residual = self.residual(x)\n",
    "        x = direct + residual\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionNet(nn.Module):\n",
    "    def __init__(self, middle_block_n = 6):\n",
    "        super(XceptionNet, self).__init__()\n",
    "\n",
    "        self.entry_block = EntryBlock()\n",
    "        self.middel_block = MiddleBlock(middle_block_n)\n",
    "        self.exit_block = ExitBlock()\n",
    "\n",
    "        self.fc = nn.Linear(1024, 136)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry_block(x)\n",
    "        x = self.middel_block(x)\n",
    "        x = self.exit_block(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XceptionNet()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('model.pt', weights_only = True))\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(save = None):\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for features, labels in tqdm(validation_loader, desc = 'Validating', ncols = 600):\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = objective(outputs, labels)\n",
    "\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "        break\n",
    "        \n",
    "    visualize_batch(features[:16].cpu(), outputs[:16].cpu(), shape = (4, 4), size = 16, title = 'Validation sample predictions', save = save)\n",
    "\n",
    "    return cum_loss/len(validation_loader)\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 50\n",
    "batches = len(training_loader)\n",
    "best_loss = np.inf\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cum_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(tqdm(training_loader, desc=f'Epoch({epoch+1}/{epochs})', ncols=800)):\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = model(features)\n",
    "        \n",
    "        loss = objective(outputs, labels) \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "    \n",
    "    train_loss = cum_loss / batches\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    \n",
    "    val_loss = validate(os.path.join('progress', f'epoch({str(epoch+1).zfill(len(str(epochs)))}).jpg'))\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        print(f'Saving model at epoch {epoch + 1}.')\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "    print(f'Epoch({epoch+1}/{epochs}) -> Training Loss: {train_loss:.8f} | Validation Loss: {val_loss:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XceptionNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('modelfinal.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = TF.to_pil_image(image)\n",
    "    image = TF.resize(image, (128, 128))\n",
    "    image = TF.to_tensor(image)\n",
    "    image = (image - image.min())/(image.max() - image.min())\n",
    "    image = (2 * image) - 1\n",
    "    return image.unsqueeze(0)\n",
    "\n",
    "def draw_landmarks_on_faces(image, faces_landmarks):\n",
    "    image = image.copy()\n",
    "    for landmarks, (left, top, height, width) in faces_landmarks:\n",
    "        landmarks = landmarks.view(-1, 2)\n",
    "        landmarks = (landmarks + 0.5)\n",
    "        landmarks = landmarks.numpy()\n",
    "        \n",
    "        for i, (x, y) in enumerate(landmarks, 1):\n",
    "            try:\n",
    "                cv2.circle(image, (int((x * width) + left), int((y * height) + top)), 2, [255, 0, 0], -1)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_image(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        crop_img = gray[y: y + h, x: x + w]\n",
    "        preprocessed_image = preprocess_image(crop_img)\n",
    "        \n",
    "        \n",
    "        landmarks_predictions = model(preprocessed_image.cuda())\n",
    "        outputs.append((landmarks_predictions.cpu(), (x, y, h, w)))\n",
    "\n",
    "    \n",
    "    return draw_landmarks_on_faces(image, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'path/to/image'\n",
    "\n",
    "output_image = inference_image(image_path)\n",
    "\n",
    "plt.figure(figsize=(11, 11))\n",
    "plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
